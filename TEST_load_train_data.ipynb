{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "ASPECT_RATIO                   1.3\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     2\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "CHANNEL_SHIFT_RANGE            15\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        512\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 2\n",
      "IMAGE_MAX_DIM                  512\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  512\n",
      "IMAGE_MIN_SCALE                False\n",
      "IMAGE_RESIZE_MODE              crop\n",
      "IMAGE_SHAPE                    [512 512   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               256\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "MIN_ENLARGE                    1.2\n",
      "NAME                           001\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "ROT_RANGE                      10.0\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                332\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           600\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               1\n",
      "WEIGHT_DECAY                   0.0001\n",
      "ZOOM                           1.5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed=123\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(seed)\n",
    "import random\n",
    "random.seed(seed)\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from mrcnn.my_bowl_dataset import BowlDataset\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "import numpy as np\n",
    "from imgaug import augmenters as iaa\n",
    "import skimage.io \n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "## SET UP CONFIGURATION\n",
    "from mrcnn.config import Config\n",
    "\n",
    "class BowlConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"001\"\n",
    "\n",
    "    IMAGE_RESIZE_MODE = \"crop\"\n",
    "    # Augmentation parameters\n",
    "    ASPECT_RATIO = 1.3 ## Maximum aspect ratio modification when scaling\n",
    "    MIN_ENLARGE = 1.2 ## Minimum enlarging of images, note that this will be randomized\n",
    "    ZOOM = 1.5 ## Maximum zoom per image, note that this will be randomized\n",
    "    IMAGE_MIN_SCALE = False ## Not using this\n",
    "\n",
    "    ROT_RANGE = 10.\n",
    "    CHANNEL_SHIFT_RANGE = 15\n",
    "    \n",
    "    LEARNING_RATE = 0.001\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 2\n",
    "\n",
    "    \n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1 # background + nuclei\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    IMAGE_MAX_DIM = 512\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8 , 16, 32, 64, 128)  # anchor side in pixels\n",
    "    # RPN_ANCHOR_SCALES = (4, 8 , 16, 32, 64)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 600\n",
    "\n",
    "    STEPS_PER_EPOCH = 664//IMAGES_PER_GPU\n",
    "    VALIDATION_STEPS = 2//IMAGES_PER_GPU ## We are training with the whole dataset so validation is not very meaningfull, I put a two here so it is faster. We either use train loss or calculate in a separate procceses the mAP for each epoch\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    # VALIDATION_STEPS = 5\n",
    "\n",
    "    USE_MINI_MASK = True\n",
    "\n",
    "    MAX_GT_INSTANCES = 256\n",
    "\n",
    "    DETECTION_MAX_INSTANCES = 512\n",
    "\n",
    "bowl_config = BowlConfig()\n",
    "bowl_config.display()\n",
    "#######################################################################################\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = '/attached/home/git_repos/DSB_2018/datasets/calcium'\n",
    "\n",
    "\n",
    "## Change this dir to the stage 1 training data\n",
    "train_dir = os.path.join(ROOT_DIR,'train')\n",
    "\n",
    "# Get train IDs\n",
    "train_ids = next(os.walk(train_dir))[1]\n",
    "train_ids = [os.path.join(train_dir, train_id) for train_id in train_ids]\n",
    "\n",
    "# Training dataset\n",
    "dataset_train = BowlDataset()\n",
    "dataset_train.load_bowl(train_ids)\n",
    "dataset_train.prepare()\n",
    "\n",
    "# # Validation dataset, same as training.. will use pad64 on this one\n",
    "dataset_val = BowlDataset()\n",
    "dataset_val.load_bowl(train_ids)\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all this is testing / inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_idx in range(13):\n",
    "    underlay = dataset_train.load_image(img_idx)\n",
    "    overlay = np.sum(dataset_train.load_mask(img_idx)[0], axis=2)\n",
    "    mixture = underlay.copy()\n",
    "    mixture[:, :, 2] = overlay*255\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 4.5))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.imshow(underlay)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax2.imshow(mixture)\n",
    "    fig.suptitle(dataset_train.image_info[img_idx]['path'], color='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229, 229, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.load_image(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.load_image(0).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 203, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.load_image(3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       dtype=int32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.load_mask(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train.load_mask(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.load_mask(0)[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.load_mask(0)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229, 229, 85)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.load_mask(0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arr = np.array(Image.open('datasets/nucleus/stage1_train/00071198d059ba7f5914a526d124d28e6d010c92466da21d4a04cd5413362552/masks/07a9bf1d7594af2763c86e93f05d22c4d5181353c6d3ab30a345b908ffe5aadc.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_arr.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### so image input is 3-channel uint8 png at between 64x64x3 and 256x256x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label input is separate binary 2D pngs for each neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### so directory structure is the following\n",
    "top folder\n",
    "|\n",
    "    each patch (or subpatch)\n",
    "    |\n",
    "        images\n",
    "        |\n",
    "            3-chan-image-name.png\n",
    "        masks\n",
    "        |\n",
    "            neuron-0-mask.png\n",
    "            neuron-1-mask.png\n",
    "            .\n",
    "            .\n",
    "            ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### back to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "## https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n",
    "COCO_MODEL_PATH = '/attached/home/git_repos/DSB_2018/datasets/nucleus/deepretina_final.h5'\n",
    "\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=bowl_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                            \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /attached/home/git_repos/DSB_2018/datasets/calcium/logs/00120190805T1913/mask_rcnn_001_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "332/332 [==============================] - 295s 888ms/step - loss: 1.6924 - rpn_class_loss: 0.1466 - rpn_bbox_loss: 0.5502 - mrcnn_class_loss: 0.1383 - mrcnn_bbox_loss: 0.4233 - mrcnn_mask_loss: 0.4339 - val_loss: 1.4092 - val_rpn_class_loss: 0.0950 - val_rpn_bbox_loss: 0.3918 - val_mrcnn_class_loss: 0.2546 - val_mrcnn_bbox_loss: 0.3152 - val_mrcnn_mask_loss: 0.3527\n",
      "Epoch 2/30\n",
      "332/332 [==============================] - 200s 601ms/step - loss: 1.5246 - rpn_class_loss: 0.1077 - rpn_bbox_loss: 0.4940 - mrcnn_class_loss: 0.1427 - mrcnn_bbox_loss: 0.3781 - mrcnn_mask_loss: 0.4021 - val_loss: 1.3991 - val_rpn_class_loss: 0.0983 - val_rpn_bbox_loss: 0.3738 - val_mrcnn_class_loss: 0.2273 - val_mrcnn_bbox_loss: 0.3337 - val_mrcnn_mask_loss: 0.3660\n",
      "Epoch 3/30\n",
      "332/332 [==============================] - 199s 599ms/step - loss: 1.4564 - rpn_class_loss: 0.0987 - rpn_bbox_loss: 0.4751 - mrcnn_class_loss: 0.1368 - mrcnn_bbox_loss: 0.3560 - mrcnn_mask_loss: 0.3898 - val_loss: 1.2970 - val_rpn_class_loss: 0.0699 - val_rpn_bbox_loss: 0.3572 - val_mrcnn_class_loss: 0.2344 - val_mrcnn_bbox_loss: 0.3006 - val_mrcnn_mask_loss: 0.3349\n",
      "Epoch 4/30\n",
      "332/332 [==============================] - 199s 600ms/step - loss: 1.4089 - rpn_class_loss: 0.0976 - rpn_bbox_loss: 0.4523 - mrcnn_class_loss: 0.1369 - mrcnn_bbox_loss: 0.3412 - mrcnn_mask_loss: 0.3809 - val_loss: 1.1598 - val_rpn_class_loss: 0.0787 - val_rpn_bbox_loss: 0.3237 - val_mrcnn_class_loss: 0.1997 - val_mrcnn_bbox_loss: 0.2500 - val_mrcnn_mask_loss: 0.3077\n",
      "Epoch 5/30\n",
      "332/332 [==============================] - 199s 598ms/step - loss: 1.3831 - rpn_class_loss: 0.0898 - rpn_bbox_loss: 0.4448 - mrcnn_class_loss: 0.1306 - mrcnn_bbox_loss: 0.3388 - mrcnn_mask_loss: 0.3791 - val_loss: 1.1288 - val_rpn_class_loss: 0.0762 - val_rpn_bbox_loss: 0.2920 - val_mrcnn_class_loss: 0.2022 - val_mrcnn_bbox_loss: 0.2515 - val_mrcnn_mask_loss: 0.3069\n",
      "Epoch 6/30\n",
      "332/332 [==============================] - 198s 595ms/step - loss: 1.3364 - rpn_class_loss: 0.0865 - rpn_bbox_loss: 0.4275 - mrcnn_class_loss: 0.1324 - mrcnn_bbox_loss: 0.3193 - mrcnn_mask_loss: 0.3706 - val_loss: 1.1524 - val_rpn_class_loss: 0.0790 - val_rpn_bbox_loss: 0.3148 - val_mrcnn_class_loss: 0.1938 - val_mrcnn_bbox_loss: 0.2450 - val_mrcnn_mask_loss: 0.3198\n",
      "Epoch 7/30\n",
      "332/332 [==============================] - 197s 595ms/step - loss: 1.3199 - rpn_class_loss: 0.0861 - rpn_bbox_loss: 0.4196 - mrcnn_class_loss: 0.1301 - mrcnn_bbox_loss: 0.3139 - mrcnn_mask_loss: 0.3702 - val_loss: 1.0712 - val_rpn_class_loss: 0.0681 - val_rpn_bbox_loss: 0.2785 - val_mrcnn_class_loss: 0.1846 - val_mrcnn_bbox_loss: 0.2368 - val_mrcnn_mask_loss: 0.3032\n",
      "Epoch 8/30\n",
      "332/332 [==============================] - 199s 598ms/step - loss: 1.2802 - rpn_class_loss: 0.0833 - rpn_bbox_loss: 0.4018 - mrcnn_class_loss: 0.1299 - mrcnn_bbox_loss: 0.3022 - mrcnn_mask_loss: 0.3631 - val_loss: 0.9479 - val_rpn_class_loss: 0.0488 - val_rpn_bbox_loss: 0.2249 - val_mrcnn_class_loss: 0.1524 - val_mrcnn_bbox_loss: 0.2249 - val_mrcnn_mask_loss: 0.2969\n",
      "Epoch 9/30\n",
      "332/332 [==============================] - 198s 596ms/step - loss: 1.2547 - rpn_class_loss: 0.0795 - rpn_bbox_loss: 0.3918 - mrcnn_class_loss: 0.1262 - mrcnn_bbox_loss: 0.2954 - mrcnn_mask_loss: 0.3618 - val_loss: 1.6913 - val_rpn_class_loss: 0.0793 - val_rpn_bbox_loss: 0.5826 - val_mrcnn_class_loss: 0.0629 - val_mrcnn_bbox_loss: 0.4610 - val_mrcnn_mask_loss: 0.5055\n",
      "Epoch 10/30\n",
      "332/332 [==============================] - 198s 596ms/step - loss: 1.2260 - rpn_class_loss: 0.0765 - rpn_bbox_loss: 0.3797 - mrcnn_class_loss: 0.1260 - mrcnn_bbox_loss: 0.2843 - mrcnn_mask_loss: 0.3595 - val_loss: 1.9516 - val_rpn_class_loss: 0.1300 - val_rpn_bbox_loss: 0.6706 - val_mrcnn_class_loss: 0.0933 - val_mrcnn_bbox_loss: 0.5380 - val_mrcnn_mask_loss: 0.5197\n",
      "Epoch 11/30\n",
      "332/332 [==============================] - 198s 596ms/step - loss: 1.2026 - rpn_class_loss: 0.0765 - rpn_bbox_loss: 0.3723 - mrcnn_class_loss: 0.1236 - mrcnn_bbox_loss: 0.2766 - mrcnn_mask_loss: 0.3537 - val_loss: 1.7615 - val_rpn_class_loss: 0.0958 - val_rpn_bbox_loss: 0.5337 - val_mrcnn_class_loss: 0.0989 - val_mrcnn_bbox_loss: 0.5131 - val_mrcnn_mask_loss: 0.5200\n",
      "Epoch 12/30\n",
      "332/332 [==============================] - 197s 593ms/step - loss: 1.1768 - rpn_class_loss: 0.0722 - rpn_bbox_loss: 0.3579 - mrcnn_class_loss: 0.1252 - mrcnn_bbox_loss: 0.2695 - mrcnn_mask_loss: 0.3520 - val_loss: 1.7640 - val_rpn_class_loss: 0.0702 - val_rpn_bbox_loss: 0.5193 - val_mrcnn_class_loss: 0.0617 - val_mrcnn_bbox_loss: 0.5808 - val_mrcnn_mask_loss: 0.5320\n",
      "Epoch 13/30\n",
      "332/332 [==============================] - 198s 597ms/step - loss: 1.1724 - rpn_class_loss: 0.0734 - rpn_bbox_loss: 0.3524 - mrcnn_class_loss: 0.1252 - mrcnn_bbox_loss: 0.2683 - mrcnn_mask_loss: 0.3531 - val_loss: 1.5935 - val_rpn_class_loss: 0.0814 - val_rpn_bbox_loss: 0.5161 - val_mrcnn_class_loss: 0.0502 - val_mrcnn_bbox_loss: 0.4524 - val_mrcnn_mask_loss: 0.4933\n",
      "Epoch 14/30\n",
      "332/332 [==============================] - 197s 594ms/step - loss: 1.1192 - rpn_class_loss: 0.0671 - rpn_bbox_loss: 0.3368 - mrcnn_class_loss: 0.1191 - mrcnn_bbox_loss: 0.2528 - mrcnn_mask_loss: 0.3434 - val_loss: 1.6731 - val_rpn_class_loss: 0.0810 - val_rpn_bbox_loss: 0.5561 - val_mrcnn_class_loss: 0.0745 - val_mrcnn_bbox_loss: 0.4602 - val_mrcnn_mask_loss: 0.5012\n",
      "Epoch 15/30\n",
      "332/332 [==============================] - 198s 595ms/step - loss: 1.1275 - rpn_class_loss: 0.0678 - rpn_bbox_loss: 0.3332 - mrcnn_class_loss: 0.1206 - mrcnn_bbox_loss: 0.2576 - mrcnn_mask_loss: 0.3483 - val_loss: 1.5582 - val_rpn_class_loss: 0.0547 - val_rpn_bbox_loss: 0.5612 - val_mrcnn_class_loss: 0.0665 - val_mrcnn_bbox_loss: 0.3817 - val_mrcnn_mask_loss: 0.4941\n",
      "Epoch 16/30\n",
      "332/332 [==============================] - 198s 595ms/step - loss: 1.0867 - rpn_class_loss: 0.0621 - rpn_bbox_loss: 0.3169 - mrcnn_class_loss: 0.1172 - mrcnn_bbox_loss: 0.2463 - mrcnn_mask_loss: 0.3442 - val_loss: 1.7110 - val_rpn_class_loss: 0.1308 - val_rpn_bbox_loss: 0.5258 - val_mrcnn_class_loss: 0.1327 - val_mrcnn_bbox_loss: 0.4321 - val_mrcnn_mask_loss: 0.4896\n",
      "Epoch 17/30\n",
      "332/332 [==============================] - 198s 596ms/step - loss: 1.0920 - rpn_class_loss: 0.0637 - rpn_bbox_loss: 0.3207 - mrcnn_class_loss: 0.1174 - mrcnn_bbox_loss: 0.2458 - mrcnn_mask_loss: 0.3445 - val_loss: 1.1138 - val_rpn_class_loss: 0.0704 - val_rpn_bbox_loss: 0.3278 - val_mrcnn_class_loss: 0.1184 - val_mrcnn_bbox_loss: 0.2538 - val_mrcnn_mask_loss: 0.3434\n",
      "Epoch 18/30\n",
      "332/332 [==============================] - 197s 594ms/step - loss: 1.0633 - rpn_class_loss: 0.0606 - rpn_bbox_loss: 0.3034 - mrcnn_class_loss: 0.1182 - mrcnn_bbox_loss: 0.2397 - mrcnn_mask_loss: 0.3414 - val_loss: 1.2855 - val_rpn_class_loss: 0.0997 - val_rpn_bbox_loss: 0.3540 - val_mrcnn_class_loss: 0.1606 - val_mrcnn_bbox_loss: 0.2908 - val_mrcnn_mask_loss: 0.3803\n",
      "Epoch 19/30\n",
      "332/332 [==============================] - 198s 598ms/step - loss: 1.0326 - rpn_class_loss: 0.0574 - rpn_bbox_loss: 0.2966 - mrcnn_class_loss: 0.1125 - mrcnn_bbox_loss: 0.2295 - mrcnn_mask_loss: 0.3366 - val_loss: 1.1344 - val_rpn_class_loss: 0.0493 - val_rpn_bbox_loss: 0.3626 - val_mrcnn_class_loss: 0.0950 - val_mrcnn_bbox_loss: 0.2696 - val_mrcnn_mask_loss: 0.3579\n",
      "Epoch 20/30\n",
      "332/332 [==============================] - 198s 596ms/step - loss: 1.0309 - rpn_class_loss: 0.0567 - rpn_bbox_loss: 0.2950 - mrcnn_class_loss: 0.1133 - mrcnn_bbox_loss: 0.2290 - mrcnn_mask_loss: 0.3369 - val_loss: 0.9658 - val_rpn_class_loss: 0.0434 - val_rpn_bbox_loss: 0.2758 - val_mrcnn_class_loss: 0.0996 - val_mrcnn_bbox_loss: 0.2132 - val_mrcnn_mask_loss: 0.3339\n",
      "Epoch 21/30\n",
      "332/332 [==============================] - 197s 594ms/step - loss: 1.0028 - rpn_class_loss: 0.0526 - rpn_bbox_loss: 0.2847 - mrcnn_class_loss: 0.1128 - mrcnn_bbox_loss: 0.2199 - mrcnn_mask_loss: 0.3328 - val_loss: 0.9365 - val_rpn_class_loss: 0.0384 - val_rpn_bbox_loss: 0.2593 - val_mrcnn_class_loss: 0.1020 - val_mrcnn_bbox_loss: 0.2006 - val_mrcnn_mask_loss: 0.3362\n",
      "Epoch 22/30\n",
      "332/332 [==============================] - 197s 594ms/step - loss: 0.9987 - rpn_class_loss: 0.0540 - rpn_bbox_loss: 0.2789 - mrcnn_class_loss: 0.1135 - mrcnn_bbox_loss: 0.2190 - mrcnn_mask_loss: 0.3334 - val_loss: 1.0176 - val_rpn_class_loss: 0.0689 - val_rpn_bbox_loss: 0.3073 - val_mrcnn_class_loss: 0.1059 - val_mrcnn_bbox_loss: 0.2010 - val_mrcnn_mask_loss: 0.3344\n",
      "Epoch 23/30\n",
      "332/332 [==============================] - 197s 592ms/step - loss: 0.9745 - rpn_class_loss: 0.0513 - rpn_bbox_loss: 0.2723 - mrcnn_class_loss: 0.1103 - mrcnn_bbox_loss: 0.2116 - mrcnn_mask_loss: 0.3289 - val_loss: 0.9746 - val_rpn_class_loss: 0.0582 - val_rpn_bbox_loss: 0.2116 - val_mrcnn_class_loss: 0.0776 - val_mrcnn_bbox_loss: 0.2530 - val_mrcnn_mask_loss: 0.3741\n",
      "Epoch 24/30\n",
      "332/332 [==============================] - 197s 595ms/step - loss: 0.9609 - rpn_class_loss: 0.0507 - rpn_bbox_loss: 0.2641 - mrcnn_class_loss: 0.1091 - mrcnn_bbox_loss: 0.2089 - mrcnn_mask_loss: 0.3281 - val_loss: 0.9541 - val_rpn_class_loss: 0.0330 - val_rpn_bbox_loss: 0.3056 - val_mrcnn_class_loss: 0.0714 - val_mrcnn_bbox_loss: 0.2372 - val_mrcnn_mask_loss: 0.3070\n",
      "Epoch 25/30\n",
      "332/332 [==============================] - 197s 594ms/step - loss: 0.9675 - rpn_class_loss: 0.0480 - rpn_bbox_loss: 0.2694 - mrcnn_class_loss: 0.1065 - mrcnn_bbox_loss: 0.2125 - mrcnn_mask_loss: 0.3310 - val_loss: 0.9554 - val_rpn_class_loss: 0.0582 - val_rpn_bbox_loss: 0.2418 - val_mrcnn_class_loss: 0.1602 - val_mrcnn_bbox_loss: 0.1865 - val_mrcnn_mask_loss: 0.3086\n",
      "Epoch 26/30\n",
      "332/332 [==============================] - 197s 592ms/step - loss: 0.9514 - rpn_class_loss: 0.0467 - rpn_bbox_loss: 0.2628 - mrcnn_class_loss: 0.1076 - mrcnn_bbox_loss: 0.2072 - mrcnn_mask_loss: 0.3271 - val_loss: 1.1549 - val_rpn_class_loss: 0.1039 - val_rpn_bbox_loss: 0.2644 - val_mrcnn_class_loss: 0.2293 - val_mrcnn_bbox_loss: 0.2203 - val_mrcnn_mask_loss: 0.3370\n",
      "Epoch 27/30\n",
      "332/332 [==============================] - 197s 595ms/step - loss: 0.9350 - rpn_class_loss: 0.0452 - rpn_bbox_loss: 0.2544 - mrcnn_class_loss: 0.1090 - mrcnn_bbox_loss: 0.2012 - mrcnn_mask_loss: 0.3252 - val_loss: 1.0723 - val_rpn_class_loss: 0.0512 - val_rpn_bbox_loss: 0.2547 - val_mrcnn_class_loss: 0.2167 - val_mrcnn_bbox_loss: 0.2177 - val_mrcnn_mask_loss: 0.3321\n",
      "Epoch 28/30\n",
      "332/332 [==============================] - 196s 591ms/step - loss: 0.9105 - rpn_class_loss: 0.0430 - rpn_bbox_loss: 0.2443 - mrcnn_class_loss: 0.1054 - mrcnn_bbox_loss: 0.1957 - mrcnn_mask_loss: 0.3221 - val_loss: 1.0126 - val_rpn_class_loss: 0.0646 - val_rpn_bbox_loss: 0.2184 - val_mrcnn_class_loss: 0.1825 - val_mrcnn_bbox_loss: 0.2236 - val_mrcnn_mask_loss: 0.3235\n",
      "Epoch 29/30\n",
      "332/332 [==============================] - 196s 591ms/step - loss: 0.9121 - rpn_class_loss: 0.0425 - rpn_bbox_loss: 0.2485 - mrcnn_class_loss: 0.1030 - mrcnn_bbox_loss: 0.1960 - mrcnn_mask_loss: 0.3222 - val_loss: 0.8843 - val_rpn_class_loss: 0.0456 - val_rpn_bbox_loss: 0.2144 - val_mrcnn_class_loss: 0.1749 - val_mrcnn_bbox_loss: 0.1603 - val_mrcnn_mask_loss: 0.2891\n",
      "Epoch 30/30\n",
      "332/332 [==============================] - 197s 594ms/step - loss: 0.8972 - rpn_class_loss: 0.0405 - rpn_bbox_loss: 0.2445 - mrcnn_class_loss: 0.1014 - mrcnn_bbox_loss: 0.1913 - mrcnn_mask_loss: 0.3194 - val_loss: 1.0023 - val_rpn_class_loss: 0.0585 - val_rpn_bbox_loss: 0.2281 - val_mrcnn_class_loss: 0.2069 - val_mrcnn_bbox_loss: 0.2012 - val_mrcnn_mask_loss: 0.3076\n",
      "\n",
      "Starting at epoch 30. LR=0.0001\n",
      "\n",
      "Checkpoint Path: /attached/home/git_repos/DSB_2018/datasets/calcium/logs/00120190805T1913/mask_rcnn_001_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "Epoch 31/50\n",
      "332/332 [==============================] - 289s 870ms/step - loss: 0.8218 - rpn_class_loss: 0.0368 - rpn_bbox_loss: 0.2140 - mrcnn_class_loss: 0.0935 - mrcnn_bbox_loss: 0.1711 - mrcnn_mask_loss: 0.3063 - val_loss: 0.5361 - val_rpn_class_loss: 0.0143 - val_rpn_bbox_loss: 0.1231 - val_mrcnn_class_loss: 0.0748 - val_mrcnn_bbox_loss: 0.0975 - val_mrcnn_mask_loss: 0.2263\n",
      "Epoch 32/50\n",
      "332/332 [==============================] - 197s 592ms/step - loss: 0.8291 - rpn_class_loss: 0.0364 - rpn_bbox_loss: 0.2205 - mrcnn_class_loss: 0.0940 - mrcnn_bbox_loss: 0.1698 - mrcnn_mask_loss: 0.3084 - val_loss: 0.5825 - val_rpn_class_loss: 0.0124 - val_rpn_bbox_loss: 0.1383 - val_mrcnn_class_loss: 0.0934 - val_mrcnn_bbox_loss: 0.0921 - val_mrcnn_mask_loss: 0.2462\n",
      "Epoch 33/50\n",
      "332/332 [==============================] - 197s 594ms/step - loss: 0.8158 - rpn_class_loss: 0.0359 - rpn_bbox_loss: 0.2173 - mrcnn_class_loss: 0.0924 - mrcnn_bbox_loss: 0.1654 - mrcnn_mask_loss: 0.3048 - val_loss: 0.4700 - val_rpn_class_loss: 0.0142 - val_rpn_bbox_loss: 0.0893 - val_mrcnn_class_loss: 0.0766 - val_mrcnn_bbox_loss: 0.0730 - val_mrcnn_mask_loss: 0.2168\n",
      "Epoch 34/50\n",
      "332/332 [==============================] - 197s 593ms/step - loss: 0.8072 - rpn_class_loss: 0.0352 - rpn_bbox_loss: 0.2136 - mrcnn_class_loss: 0.0925 - mrcnn_bbox_loss: 0.1626 - mrcnn_mask_loss: 0.3032 - val_loss: 0.5636 - val_rpn_class_loss: 0.0164 - val_rpn_bbox_loss: 0.1387 - val_mrcnn_class_loss: 0.0818 - val_mrcnn_bbox_loss: 0.0936 - val_mrcnn_mask_loss: 0.2331\n",
      "Epoch 35/50\n",
      "332/332 [==============================] - 197s 595ms/step - loss: 0.7885 - rpn_class_loss: 0.0339 - rpn_bbox_loss: 0.2066 - mrcnn_class_loss: 0.0894 - mrcnn_bbox_loss: 0.1586 - mrcnn_mask_loss: 0.3001 - val_loss: 0.5297 - val_rpn_class_loss: 0.0095 - val_rpn_bbox_loss: 0.1211 - val_mrcnn_class_loss: 0.0766 - val_mrcnn_bbox_loss: 0.0851 - val_mrcnn_mask_loss: 0.2374\n",
      "Epoch 36/50\n",
      "233/332 [====================>.........] - ETA: 58s - loss: 0.7943 - rpn_class_loss: 0.0337 - rpn_bbox_loss: 0.2115 - mrcnn_class_loss: 0.0896 - mrcnn_bbox_loss: 0.1582 - mrcnn_mask_loss: 0.3012"
     ]
    }
   ],
   "source": [
    "augmentation=False\n",
    "\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=bowl_config.LEARNING_RATE,\n",
    "            epochs=30,\n",
    "            augmentation=augmentation,\n",
    "            augment=True,\n",
    "            layers=\"all\")\n",
    "\n",
    "\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=bowl_config.LEARNING_RATE/10,\n",
    "            epochs=50,\n",
    "            augmentation=augmentation,\n",
    "            augment=True,\n",
    "            layers=\"all\")\n",
    "\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=bowl_config.LEARNING_RATE/30,\n",
    "            epochs=75,\n",
    "            augmentation=augmentation,\n",
    "            augment=True,\n",
    "            layers=\"all\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "ellapsed_time = (end_time-start_time)/3600\n",
    "\n",
    "print(model.log_dir)\n",
    "model_path = os.path.join(model.log_dir, 'final.h5')\n",
    "model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
